\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage[pdftex]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage} 
\usepackage{caption}
\usepackage{subcaption}

\begin{document}  

\title{ACRL Homework 1:  Tetris}
\date{February 25, 2014}
\author{Eleanor Avrunin, Humphrey Hu, Kumar Shaurya Shankar}

\maketitle

\section{Choice of Algorithm and Motivation}
We approached the problem task by using Policy Gradients. Our main motivation was that we wanted to exploit the knowledge of the structure of the system due to the availability of cheap, inexpensive forward simulation available to us via the game simulator. 


\section{Algorithm}
We implemented a variant general Policy Gradient Theorem inspired by the REINFORCE algorithm. We incorporate causality by only calculating rewards that are obtained after the current timestep, since the actions that are chosen at a particular timestep cannot influence the rewards in the past. The reward function to go is thus just the sum of individual rewards over the length of the trajectory till death. The algorithm is as follows

\begin{algorithm}
\caption{Modified REINFORCE}
\label{reinforce_alg}
\begin{algorithmic}
\FOR {$i = 1\dots n_{\text{fitting iterations}}$}
\STATE Run simulator with $\pi_{\theta}$ to collect $\xi^{1...N}$\\
\FOR {$j = 1 \dots N$}
\STATE {$z_1 \gets \vec{0}, ~ \Delta_1 \gets \vec{0}$}\\
\FOR {$t = 1 \dots T-1$}
%\STATE  $\nabla_{\theta}J \gets \frac{1}{N}\underset{i=1}{\overset{N}{\sum}}\left[\sum\limits_{t=0}^{T-1}\nabla_{\theta}\log\pi\left(a_{t}^{i}|s_{t}^{i}\right) \sum\limits_{t}^{T-1} r_t\right]$\\
%\STATE $\theta_{new} \gets \theta_{old}+\alpha\nabla_{\theta}J$
\STATE $z_{t+1} \gets \gamma z_t + \nabla_{\theta}\log\pi\left(a_{t}^{i}|s_{t}^{i} \right)$
\STATE $\Delta_{t+1} \gets \Delta_t + \frac{1}{t+1} \left( z_{t+1} r_t - \Delta_t\right)$
\ENDFOR
\STATE $\delta(j) \gets \Delta_T$
\ENDFOR
\STATE $\hat{\mathbf{\delta}} \gets \frac{1}{N} \sum_{i=1}^{N} \delta(j)$
%\STATE $\hat{\Delta} \gets \mathbf{\delta} - \Delta $
\STATE $\theta_{new} \gets  \theta_{old}+\alpha\hat{\mathbf{\delta}}$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Features}

The features we use are all based on the state of the board after taking an action.  For this problem, there is no difference between two actions other than the resulting state, so we do not have any action-based features.  This might not be the case for a robot problem, for instance, where different motion or sensing actions could have different associated costs.

We also do not compute any features from the next piece in the resulting state.  Because the next piece is stochastically generated by the simulator, forward-simulating different possible actions would produce different next pieces unrelated to the quality of the action.  We could have considered the results for all of the possible next pieces and the actions that could then be taken with them (i.e., done depth-2 lookahead), but that would increase the computation needed at each step by an order of magnitude.  We decided that the added slowness was not worth the possible improvements.

In looking at the board after taking an action, we consider the features used by human Tetris players.  In general, these are the height of the board, how full the space is, and how many rows can be cleared, which we then separate into a number of individual features.  The height of the board is the primary factor in how likely the player is to die soon, and the density of the board determines both how likely it is that more rows can be cleared (higher reward) and how likely it is that the height will increase.

For the height of the board, our features are based on the individual column heights, accessed using the \texttt{getTop()} method provided.  We include as features the height of the tallest and shortest columns, and the average height of the columns.

We describe the density of the board using the number of filled squares in the rows.  As with the columns, we have features for the size of the largest and smallest rows.  We also pay particular attention to the empty squares below where the agent has placed pieces.  We compute the total number of holes (empty squares with a filled square higher in the same column) in the board.  As another measure of density and available space, we also compute the number of empty squares below the highest filled square in the board, whether or not there is a filled square above them.  This distinguishes boards that are fairly evenly filled from boards of the same height that have a tower of pieces in one small area.

Our final feature is the number of rows cleared by making the given move.

We also tried a number of other features.  One such feature was the number of filled squares in the top four rows of the board, which was meant to strongly discourage the agent getting close to the end of the game.  We tried several different measures of density, such as the number of filled squares in the highest row with a piece and the average number of filled squares in a row (over both the whole board and the region with pieces).  Our results with these additional features were slightly worse than with the feature set described above, so we discarded them.  In addition, we considered including the number of overhangs (filled squared with an empty square below them, the counterpart to holes) as a feature, but in the end decided it was not likely to produce a significant improvement when we already included the number of holes as a feature.

We sped up the computation of the features by making a binary copy of the board, where filled squares have value $1$ instead of the ID of the piece at that location.  This allows us to compute the number of filled squares in a row or column by summation, and find the gaps by subtraction from the total number of squares in a row or the height of the column.


\section{Implementation Details}
For the simulation we use the Java based simulator provided in the assignment. The system architecture consists of generic interfaces that are implemented as class variants for various purposes for modularity. For generating trajectories we generate random trajectories by concurrently forward simulating using multiple threads for efficiency.

The policy distribution function is designed to be a Boltzmann distribution that operates on a linear function of features, i.e.,
\[ \pi(a|s)=\frac{\exp\left(\theta^{T}f\left(s_{a}^{\prime}\right)\right)}{\underset{a^{\prime}}{\sum}\exp\left(\theta^{T}f\left(s_{a^{\prime}}^{\prime}\right)\right)} \]
 The policy gradient parameters are initialized with uniform random weights, and the starting policy for generating trajectories is a random policy. Since the first few moves do not contribute substantial immediate rewards starting from a blank board, we play a fixed number of moves as per a random policy first before forward simulating 100 trajectorie. For every iteration of our training, we fit the policy to these 100 trajectories using the algorithm stated above. The updated parameters are then used to generate the next 100 trajectories in the next iteration, and we continue till we don't see any change in gradients. At every iteration, we save the current weight parameters to disk and at the time of execution, we read the cached parameters and use them in the Agent class to predict the action to be taken.
 
Since the policy is stochastic, we pick an action to choose by first generating the probability distribution of picking an action given a state, and then pick the action by sampling from this distribution. The distribution is generated by using the current saved parameters of the policy in the Boltzmann distribution definition as defined above. Since naively taking the exponential of the number can lead to overflow errors, we cater for that by normalizing the exponent using the log likelihood of the action. We also perform Laplacian smoothing to the probability distribution for avoiding numerical brittleness in evaluating the gradient. The sampling is done by picking a random number from a uniform distribution and finding the inverse of the cumulative distribution function that it corresponds to.


\section{Results and Discussion}
For the implemented method, we obtain an average number of rows cleared of .

It is interesting to note that implementing policy gradient hasn't worked as well as we had anticipated. The primary issue we face is that the gradient as determined is not very informative, and as such, even after averaging we do not obtain any improvement after the initial improvement in the learned policy. Perhaps the features do not encapsulate the state space sufficiently, or are not discriminative enough. It could also be that the policy function that we are trying to fit on this set of features does not have the right descriptive power. 

\end{document}