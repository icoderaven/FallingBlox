\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage[pdftex]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage} 
\usepackage{caption}
\usepackage{subcaption}

\begin{document}  

\title{ACRL Homework 1:  Tetris}
\date{February 25, 2014}
\author{Eleanor Avrunin, Humphrey Hu, Kumar Shaurya Shankar}

\maketitle

\section{Algorithm}
We approach the task of solving the problem using the general Policy Gradient Theorem inspired by the REINFORCE algorithm. Unlike the former, however, we incorporate causality by only calculating rewards that are obtained after the current timestep, since the actions that are chosen at a particular timestep cannot influence the rewards in the past. The algorithm is as follows

\begin{algorithm}
\caption{Modified REINFORCE}
\label{reinforce_alg}
\begin{algorithmic}
\FOR {$i = 1\dots n_{\text{fitting iterations}}$}
\STATE Run simulator with $\pi_{\theta}$ to collect $\xi^{1...N}$\\
\FOR {$j = 1 \dots N$}
\STATE {$z_1 \gets \vec{0}, ~ \Delta_1 \gets \vec{0}$}\\
\FOR {$t = 1 \dots T-1$}
%\STATE  $\nabla_{\theta}J \gets \frac{1}{N}\underset{i=1}{\overset{N}{\sum}}\left[\sum\limits_{t=0}^{T-1}\nabla_{\theta}\log\pi\left(a_{t}^{i}|s_{t}^{i}\right) \sum\limits_{t}^{T-1} r_t\right]$\\
%\STATE $\theta_{new} \gets \theta_{old}+\alpha\nabla_{\theta}J$
\STATE $z_{t+1} \gets \gamma z_t + \nabla_{\theta}\log\pi\left(a_{t}^{i}|s_{t}^{i} \right)$
\STATE $\Delta_{t+1} \gets \Delta_t + \frac{1}{t+1} \left( z_{t+1}\sum\limits_{t+1}^{T-1} r_t - \Delta_t\right)$
\ENDFOR
\STATE $\delta(j) \gets \theta_{old}+\alpha \Delta_T$
\ENDFOR
\STATE $\theta_{new} \gets  avg(\delta)$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Features}

The features we use are all based on the state of the board after taking an action.  For this problem, there is no difference between two actions other than the resulting state, so we do not have any action-based features.  This might not be the case for a robot problem, for instance, where different motion or sensing actions could have different associated costs.

We also do not compute any features from the next piece in the resulting state.  Because the next piece is stochastically generated by the simulator, forward-simulating different possible actions would produce different next pieces unrelated to the quality of the action.  We could have considered the results for all of the possible next pieces and the actions that could then be taken with them (i.e., done depth-2 lookahead), but that would increase the computation needed at each step by an order of magnitude.  We decided that the added slowness was not worth the possible improvements.

In looking at the board after taking an action, we consider the features used by human Tetris players.  In general, these are the height of the board, how full the space is, and how many rows can be cleared, which we then separate into a number of individual features.  The height of the board is the primary factor in how likely the player is to die soon, and the density of the board determines both how likely it is that more rows can be cleared (higher reward) and how likely it is that the height will increase.

For the height of the board, our features are based on the individual column heights, accessed using the \texttt{getTop()} method provided.  We include as features the height of the tallest and shortest columns, and the average height of the columns.

We describe the density of the board using the number of filled squares in the rows.  As with the columns, we have features for the size of the largest and smallest row and the average number of filled squares in a row.  We also pay particular attention to the empty squares below where the agent has placed pieces.  We compute the total number of holes (empty squares with a filled square higher in the same column) in the board.  As another measure of density and available space, we also compute the number of empty squares below the highest filled square in the board, whether or not there is a filled square above them.

Our final feature is the number of rows cleared by making this particular move.

We considered also including other measures of board density, such as the number of overhangs (filled squared with an empty square below them, the counterpart to holes), the average number of filled squares in rows below the top row, or the number of filled squares in the top row, but in the end decided they were not likely to produce significant improvement over our existing feature set.

We sped up the computation of the features by making a binary copy of the board, where filled squares have value $1$ instead of the ID of the piece at that location.  This allows us to compute the number of filled squares in a row or column by summation, and find the gaps by subtraction from the total number of squares in a row or the height of the column.


\section{Implementation Details}
For the simulation we use the Java based simulator provided in the assignment. The system architecture consists of generic interfaces that are implemented as class variants for various purposes for modularity. For generating trajectories we generate random trajectories by concurrently forward simulating using multiple threads for efficiency.

The policy distribution function is designed to be a Boltzmann distribution that operates on a linear function of features, i.e.,
\[ \pi(a|s)=\frac{\exp\left(\theta^{T}f\left(s_{a}^{\prime}\right)\right)}{\underset{a^{\prime}}{\sum}\exp\left(\theta^{T}f\left(s_{a^{\prime}}^{\prime}\right)\right)} \]
 The policy gradient parameters are initialized with uniform random weights, and the starting policy for generating trajectories is a random policy.

% Normalizations issues?

\section{Results}








\end{document}