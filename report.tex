\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage[pdftex]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage} 
\usepackage{caption}
\usepackage{subcaption}

\begin{document}  

\title{ACRL Homework 1:  Tetris}
\date{February 25, 2014}
\author{Eleanor Avrunin, Humphrey Hu, Kumar Shaurya Shankar}

\maketitle

\section{Algorithm}
We approach the task of solving the problem using the general Policy Gradient Theorem inspired by the REINFORCE algorithm. Unlike the former, however, we incorporate causality by only calculating rewards that are obtained after the current timestep, since the actions that are chosen at a particular timestep cannot influence the rewards in the past. The algorithm is as follows

\begin{algorithm}
\caption{Modified REINFORCE}
\label{reinforce_alg}
\begin{algorithmic}
\FOR {$i = 1\dots n_{\text{fitting iterations}}$}
\STATE Run simulator with $\pi_{\theta}$ to collect $\xi^{1...N}$\\
\FOR {$j = 1 \dots N$}
\STATE {$z_1 \gets \vec{0}, ~ \Delta_1 \gets \vec{0}$}\\
\FOR {$t = 1 \dots T-1$}
%\STATE  $\nabla_{\theta}J \gets \frac{1}{N}\underset{i=1}{\overset{N}{\sum}}\left[\sum\limits_{t=0}^{T-1}\nabla_{\theta}\log\pi\left(a_{t}^{i}|s_{t}^{i}\right) \sum\limits_{t}^{T-1} r_t\right]$\\
%\STATE $\theta_{new} \gets \theta_{old}+\alpha\nabla_{\theta}J$
\STATE $z_{t+1} \gets \gamma z_t + \nabla_{\theta}\log\pi\left(a_{t}^{i}|s_{t}^{i} \right)$
\STATE $\Delta_{t+1} \gets \Delta_t + \frac{1}{t+1} \left( z_{t+1}\sum\limits_{t+1}^{T-1} r_t - \Delta_t\right)$
\ENDFOR
\STATE $\delta(j) \gets \theta_{old}+\alpha \Delta_T$
\ENDFOR
\STATE $\theta_{new} \gets  avg(\delta)$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Features}

The features we use are all based on the state of the board after taking an action.  For this problem, there is no difference between two actions other than the resulting state, so we do not have any action-based features.  This might not be the case for a robot problem, for instance, where different motion or sensing actions could have different associated costs.

We also do not compute any features from the next piece in the resulting state.  Because the next piece is stochastically generated by the simulator, forward-simulating different possible actions would produce different next pieces unrelated to the quality of the action.  We could have considered the results for all of the possible next pieces and the actions that could then be taken with them (i.e., done depth-2 lookahead), but that would increase the number of feature vectors we need to compute at each step by roughly a factor of $7$.  We decided that the added slowness was not worth the possible improvements.

In looking at the board after taking an action, we considered the features used by human Tetris players.  In general, these are the height of the board and how full the space is, which we then separate into a number of individual features.  For the height of the board, we have a feature for each column's individual height, accessed using the \texttt{getTop()} method provided.  We also include separate features for the height of the tallest and shortest columns, and the average height of the columns.

We describe the density of the board using the number of filled squares in each row.  As with the columns, we also have separate features for the size of the largest and smallest row and the average number of filled squared in a row.  We also pay particular attention to the empty squares below where the agent has placed pieces. % holes and nEmptyBelow; considered average only below top piece, filled spaces in top row, number of overhangs, but decided not to implement them

% nRowsEroded

All of our features are normalized by the number of squares in a row or column, or in the entire board, as appropriate.

We speed up the computation of the features by making a binary copy of the board, where filled squares have value $1$ instead of the ID of the piece at that location.  This allows us to compute the number of filled squares in a row or column by addition, and find the gaps by subtraction from the total number of squares in a row or the height of the column.


\section{Implementation Details}
For the simulation we use the Java based simulator provided in the assignment. The system architecture consists of generic interfaces that are implemented as class variants for various purposes for modularity. For generating trajectories we generate random trajectories by concurrently forward simulating using multiple threads for efficiency.

The policy distribution function is designed to be a Boltzmann distribution that operates on a linear function of features, i.e.,
\[ \pi(a|s)=\frac{\exp\left(\theta^{T}f\left(s_{a}^{\prime}\right)\right)}{\underset{a^{\prime}}{\sum}\exp\left(\theta^{T}f\left(s_{a^{\prime}}^{\prime}\right)\right)} \]
 The policy gradient parameters are initialized with uniform random weights, and the starting policy for generating trajectories is a random policy.

% Normalizations issues?

\section{Results}








\end{document}